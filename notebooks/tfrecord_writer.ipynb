{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aadd3254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b0b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for reading and writing\n",
    "data_dir = pathlib.Path(r\"C:\\Users\\hemert\\OneDrive - Stichting Deltares\\Desktop\\Projects\\ET_FSO\\Snellius\\Data\")\n",
    "model_dir = pathlib.Path(r\"C:\\Users\\hemert\\OneDrive - Stichting Deltares\\Desktop\\Projects\\ET_FSO\\Snellius\\VAE Model\")\n",
    "write_train_dir = pathlib.Path(r\"C:\\Users\\hemert\\OneDrive - Stichting Deltares\\Desktop\\Projects\\ET_FSO\\Snellius\\Data\\tfrecords_train\")\n",
    "write_val_dir = pathlib.Path(r\"C:\\Users\\hemert\\OneDrive - Stichting Deltares\\Desktop\\Projects\\ET_FSO\\Snellius\\Data\\tfrecords_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1555b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant data to convert to tfrecord files\n",
    "generator_data = pd.read_feather(data_dir.joinpath(\n",
    "    \"generator_data_simple_10numerics_wrecalc_allBasins_no_extremes.feather\"))\n",
    "index2word = pd.read_feather(data_dir.joinpath(\n",
    "    \"index2word_10numerics.feather\")).iloc[0].astype(str).to_list()\n",
    "tf_dist = pd.read_feather(data_dir.joinpath(\n",
    "    \"functions_simple_10_numerics_Distribution_indiv_scale_wrecalc_allBasins_no_extremes.feather\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b0c1cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale distribution values\n",
    "def min_max_scale(x):\n",
    "    return np.round((x - np.min(x)) / (np.max(x) - np.min(x)), 11)\n",
    "\n",
    "to_drop = [\"transfer_function\", \"min\", \"max\"]\n",
    "dist_scaled = tf_dist.copy().drop(to_drop, axis=1)\n",
    "for name in dist_scaled.columns:\n",
    "    dist_scaled[name] = min_max_scale(dist_scaled[name])\n",
    "\n",
    "# Train/val split\n",
    "np.random.seed(0)\n",
    "train_ind = np.random.choice(tf_dist.index, int(0.8*tf_dist.shape[0]), replace=False)\n",
    "np.save(model_dir.joinpath(\"train_ind_no_extremes.npy\"), train_ind)\n",
    "\n",
    "x_train_tf = np.expand_dims(generator_data.iloc[train_ind].values, axis=-1)\n",
    "x_val_tf = np.expand_dims(generator_data.drop(train_ind).values, axis=-1)\n",
    "\n",
    "y_train_tf = np.expand_dims(generator_data.iloc[train_ind].values, axis=-1)\n",
    "y_val_tf = np.expand_dims(generator_data.drop(train_ind).values, axis=-1)\n",
    "\n",
    "train_dist = dist_scaled.iloc[train_ind].values\n",
    "val_dist = dist_scaled.drop(train_ind).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "009a1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(data_path):\n",
    "    files = tf.io.gfile.glob(data_path + \"/\" + \"*.tfrecords\")\n",
    "    return files\n",
    "\n",
    "def get_dataset(files):\n",
    "    \"\"\"return a tfrecord dataset with all tfrecord files\"\"\"\n",
    "    dataset =  tf.data.TFRecordDataset(files)\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df7199a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        # BytesList won't unpack a string from an EagerTensor.\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def serialize_array(array):\n",
    "    array = tf.io.serialize_tensor(array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "449ee032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_combined_data(x, y, dist):\n",
    "\n",
    "    # define the dictionary -- the structure -- of our single example\n",
    "    data = {}\n",
    "    \n",
    "    # define dictionary for each mode\n",
    "    data['x'] = _bytes_feature(serialize_array(x))\n",
    "    data['y'] = _bytes_feature(serialize_array(y))\n",
    "    data['dist'] = _bytes_feature(serialize_array(dist))\n",
    "\n",
    "    out = tf.train.Example(features=tf.train.Features(feature=data))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dae4927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(x, y, dist,\n",
    "               filename, max_files, out_dir):\n",
    "    '''Writes the data to multiple tfrecord files each containing max_files examples'''\n",
    "    n_samples = len(x)\n",
    "    splits = (n_samples//max_files) + 1\n",
    "    if n_samples % max_files == 0:\n",
    "        splits -= 1\n",
    "\n",
    "    print(\n",
    "        f\"\\nUsing {splits} shard(s) for {n_samples} files,\\\n",
    "            with up to {max_files} samples per shard\")\n",
    "\n",
    "    file_count = 0\n",
    "\n",
    "    for i in tqdm.tqdm(range(splits)):\n",
    "        if i == splits - 1 and n_samples % max_files != 0:\n",
    "            current_shard_name = \"{}{}_{}{}_{}.tfrecords\".format(\n",
    "                out_dir, i+1, splits, filename, n_samples % max_files)\n",
    "        else:\n",
    "            current_shard_name = \"{}{}_{}{}_{}.tfrecords\".format(\n",
    "                out_dir, i+1, splits, filename, max_files)\n",
    "        writer = tf.io.TFRecordWriter(current_shard_name)\n",
    "\n",
    "        current_shard_count = 0\n",
    "        while current_shard_count < max_files:\n",
    "            index = i*max_files + current_shard_count\n",
    "            if index == n_samples:\n",
    "                break\n",
    "            \n",
    "            current_x = x[index]\n",
    "            current_y = y[index]\n",
    "            current_dist = dist[index]\n",
    "            \n",
    "            out = parse_combined_data(x=current_x,                                    \n",
    "                                      y=current_y,\n",
    "                                      dist=current_dist)\n",
    "\n",
    "            \n",
    "            writer.write(out.SerializeToString())\n",
    "            current_shard_count += 1\n",
    "            file_count += 1\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "    print(f\"\\nWrote {file_count} elements to TFRecord\")\n",
    "    return file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43e46f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using 3598 shard(s) for 3597769 files,            with up to 1000 samples per shard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3598/3598 [52:40<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote 3597769 elements to TFRecord\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3597769"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write training data to tfrecords\n",
    "write_data(\n",
    "    x_train_tf,\n",
    "    y_train_tf,\n",
    "    train_dist, \n",
    "    max_files=1000,\n",
    "    filename='train-tfrecords-v1', \n",
    "    out_dir=str(write_train_dir) + '/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42f2f542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using 900 shard(s) for 899443 files,            with up to 1000 samples per shard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 900/900 [14:04<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote 899443 elements to TFRecord\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "899443"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write validation data to tfrecords\n",
    "write_data(\n",
    "    x_val_tf,\n",
    "    y_val_tf,\n",
    "    val_dist, \n",
    "    max_files=1000,\n",
    "    filename='val-tfrecords-v1', \n",
    "    out_dir=str(write_val_dir) + '/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(data_path):\n",
    "    files = tf.io.gfile.glob(data_path + \"/\" + \"*.tfrecords\")\n",
    "    return files\n",
    "\n",
    "def get_dataset(files):\n",
    "    \"\"\"return a tfrecord dataset with all tfrecord files\"\"\"\n",
    "    dataset =  tf.data.TFRecordDataset(files)\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb2de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_parse(eg):\n",
    "    \"\"\"parse an example (or batch of examples, not quite sure...)\"\"\"\n",
    "\n",
    "    # here we re-specify our format\n",
    "    # you can also infer the format from the data using tf.train.Example.FromString\n",
    "    # but that did not work\n",
    "    example = tf.io.parse_example(\n",
    "        eg[tf.newaxis],\n",
    "        {\n",
    "            'x': tf.io.FixedLenFeature([], tf.string),\n",
    "            'y': tf.io.FixedLenFeature([], tf.string),\n",
    "            'dist': tf.io.FixedLenFeature([], tf.string),\n",
    "        },\n",
    "    )\n",
    "    x_tf = tf.io.parse_tensor(example[\"x\"][0], out_type=\"int32\")\n",
    "    y_tf = tf.io.parse_tensor(example[\"y\"][0], out_type=\"int32\")\n",
    "    dist = tf.io.parse_tensor(example[\"dist\"][0], out_type=\"float64\")\n",
    "\n",
    "    x_tf = tf.cast(tf.ensure_shape(x_tf, (32, None)), tf.float64)\n",
    "    y_tf = tf.cast(tf.ensure_shape(y_tf, (32, None)), tf.float64)\n",
    "    dist = tf.ensure_shape(dist, 9)\n",
    "    return (x_tf, dist), (y_tf, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc36ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tfrecords file for testing\n",
    "train_files = get_files(str(train_data_dir))\n",
    "val_files  = get_files(str(val_data_dir))\n",
    "\n",
    "train_dataset = get_dataset(train_files).shuffle(1).batch(batch_size, drop_remainder=True)\n",
    "val_dataset = get_dataset(val_files).shuffle(1).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6fa3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
